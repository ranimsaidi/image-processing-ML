{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranimsaidi/image-processing-ML/blob/main/Copie_de_apr%C3%A9s_r%C3%A9haussement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDG74JxyUryV"
      },
      "outputs": [],
      "source": [
        "def lbp_extractor(pic):\n",
        "  from skimage import feature \n",
        "  import numpy as np\n",
        "  import cv2\n",
        "  \n",
        "  \n",
        "\n",
        "  def lbp_features(img):\n",
        "    \n",
        "    # LBP operates in images of only one channel, here we go converter\n",
        "    \n",
        "    # RGB for cinza scaling using the Luminance method\n",
        "     #= img[:,:,0].astype(float)*0.3 + img[:,:,1].astype(float)*0.59 + img[:,:,2].astype(float)*0.11\n",
        "    img_gray= cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    \n",
        "    # here we define the number of points and the radius, padrao = 8, 1\n",
        "    lbp = feature.local_binary_pattern(img_gray.astype(np.uint8), 3, 3, method=\"uniform\")\n",
        "    \n",
        "    # lbp returns a matrix with the codes, so we must extract the histogram\n",
        "    (hist, _) = np.histogram(lbp, bins=2**12)\n",
        "\n",
        "    # normalize or histogram\n",
        "    hist = hist.astype(\"float\")\n",
        "    hist /= (hist.sum() + 1e-6)\n",
        "    # return the histogram of Local Binary Patterns\n",
        "\n",
        "    return hist\n",
        "  img1 = \"ref_verylow.png\"\n",
        "  img2 = \"ref_low.png\"\n",
        "  img3 = \"ref_medium.png\"\n",
        "  img4 = \"ref_high.png\"  \n",
        "  lbp1 = lbp_features(img1)\n",
        "  lbp2 = lbp_features(img2)\n",
        "  lbp3 = lbp_features(img3)\n",
        "  lbp4 = lbp_features(img4) \n",
        "  \n",
        "  lbp = lbp_features(pic)\n",
        "  score1=cv2.compareHist(lbp.astype(np.float32),lbp1.astype(np.float32), cv2.HISTCMP_CORREL)  \n",
        "  score2=cv2.compareHist(lbp.astype(np.float32),lbp2.astype(np.float32),  cv2.HISTCMP_CORREL)       \n",
        "  score3=cv2.compareHist(lbp.astype(np.float32),lbp3.astype(np.float32),  cv2.HISTCMP_CORREL)  \n",
        "  score4=cv2.compareHist(lbp.astype(np.float32),lbp4.astype(np.float32),  cv2.HISTCMP_CORREL) \n",
        "  return score1,score2,score3,score4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOoLy4AqU1Uo"
      },
      "outputs": [],
      "source": [
        "def hog_extractor(img):\n",
        "\n",
        " import cv2\n",
        " import numpy as np\n",
        " def hog_features(pic):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from skimage import io\n",
        "    from skimage import color\n",
        "    from skimage.transform import resize\n",
        "    import math\n",
        "    from skimage.feature import hog\n",
        "    import numpy as np\n",
        "    \n",
        "    img = resize(color.rgb2gray(io.imread(pic)), (128, 64))\n",
        "    \n",
        "    \n",
        "    img = np.array(img)\n",
        "    \n",
        "    \n",
        "    mag = []\n",
        "    theta = []\n",
        "    for i in range(128):\n",
        "      magnitudeArray = []\n",
        "      angleArray = []\n",
        "      for j in range(64):\n",
        "        # Condition for axis 0\n",
        "        if j-1 <= 0 or j+1 >= 64:\n",
        "          if j-1 <= 0:\n",
        "            # Condition if first element\n",
        "            Gx = img[i][j+1] - 0\n",
        "          elif j + 1 >= len(img[0]):\n",
        "            Gx = 0 - img[i][j-1]\n",
        "        # Condition for first element\n",
        "        else:\n",
        "          Gx = img[i][j+1] - img[i][j-1]\n",
        "        \n",
        "        # Condition for axis 1\n",
        "        if i-1 <= 0 or i+1 >= 128:\n",
        "          if i-1 <= 0:\n",
        "            Gy = 0 - img[i+1][j]\n",
        "          elif i +1 >= 128:\n",
        "            Gy = img[i-1][j] - 0\n",
        "        else:\n",
        "          Gy = img[i-1][j] - img[i+1][j]\n",
        "    \n",
        "        # Calculating magnitude\n",
        "        magnitude = math.sqrt(pow(Gx, 2) + pow(Gy, 2))\n",
        "        magnitudeArray.append(round(magnitude, 9))\n",
        "    \n",
        "        # Calculating angle\n",
        "        if Gx == 0:\n",
        "          angle = math.degrees(0.0)\n",
        "        else:\n",
        "          angle = math.degrees(abs(math.atan(Gy / Gx)))\n",
        "        angleArray.append(round(angle, 9))\n",
        "      mag.append(magnitudeArray)\n",
        "      theta.append(angleArray)\n",
        "    \n",
        "    mag = np.array(mag)\n",
        "    \n",
        "    \n",
        "    theta = np.array(theta)\n",
        "    \n",
        "    number_of_bins = 9\n",
        "    step_size = 180 / number_of_bins\n",
        "    \n",
        "    def calculate_j(angle):\n",
        "      temp = (angle / step_size) - 0.5\n",
        "      j = math.floor(temp)\n",
        "      return j\n",
        "    \n",
        "    def calculate_Cj(j):\n",
        "      Cj = step_size * (j + 0.5)\n",
        "      return round(Cj, 9)\n",
        "    \n",
        "    def calculate_value_j(magnitude, angle, j):\n",
        "      Cj = calculate_Cj(j+1)\n",
        "      Vj = magnitude * ((Cj - angle) / step_size)\n",
        "      return round(Vj, 9)\n",
        "    \n",
        "    histogram_points_nine = []\n",
        "    for i in range(0, 128, 8):\n",
        "      temp = []\n",
        "      for j in range(0, 64, 8):\n",
        "        magnitude_values = [[mag[i][x] for x in range(j, j+8)] for i in range(i,i+8)]\n",
        "        angle_values = [[theta[i][x] for x in range(j, j+8)] for i in range(i, i+8)]\n",
        "        for k in range(len(magnitude_values)):\n",
        "          for l in range(len(magnitude_values[0])):\n",
        "            bins = [0.0 for _ in range(number_of_bins)]\n",
        "            value_j = calculate_j(angle_values[k][l])\n",
        "            Vj = calculate_value_j(magnitude_values[k][l], angle_values[k][l], value_j)\n",
        "            Vj_1 = magnitude_values[k][l] - Vj\n",
        "            bins[value_j]+=Vj\n",
        "            bins[value_j+1]+=Vj_1\n",
        "            bins = [round(x, 9) for x in bins]\n",
        "        temp.append(bins)\n",
        "      histogram_points_nine.append(temp)\n",
        "    \n",
        "    epsilon = 1e-05\n",
        "    \n",
        "    feature_vectors = []\n",
        "    for i in range(0, len(histogram_points_nine) - 1, 1):\n",
        "      temp = []\n",
        "      for j in range(0, len(histogram_points_nine[0]) - 1, 1):\n",
        "        values = [[histogram_points_nine[i][x] for x in range(j, j+2)] for i in range(i, i+2)]\n",
        "        final_vector = []\n",
        "        for k in values:\n",
        "          for l in k:\n",
        "            for m in l:\n",
        "              final_vector.append(m)\n",
        "        k = round(math.sqrt(sum([pow(x, 2) for x in final_vector])), 9)\n",
        "        final_vector = [round(x/(k + epsilon), 9) for x in final_vector]\n",
        "        temp.append(final_vector)\n",
        "      feature_vectors.append(temp)\n",
        "    \n",
        "    \n",
        "    hogg_features = np.array(feature_vectors)\n",
        "    \n",
        "    hog_reshaped=hogg_features.reshape(3780,)\n",
        "    return hog_reshaped\n",
        " img1 = \"ref_verylow.png\"\n",
        " img2 = \"ref_low.png\"\n",
        " img3 = \"ref_medium.png\"\n",
        " img4 = \"ref_high.png\"  \n",
        " hog1 = hog_features(img1)\n",
        " hog2 = hog_features(img2)\n",
        " hog3 = hog_features(img3)\n",
        " hog4 =hog_features(img4) \n",
        " hogg = hog_features(img)\n",
        " score1=cv2.compareHist(hogg.astype(np.float32),hog1.astype(np.float32), cv2.HISTCMP_CORREL)  \n",
        " score2=cv2.compareHist(hogg.astype(np.float32),hog2.astype(np.float32),  cv2.HISTCMP_CORREL)       \n",
        " score3=cv2.compareHist(hogg.astype(np.float32),hog3.astype(np.float32),  cv2.HISTCMP_CORREL)  \n",
        " score4=cv2.compareHist(hogg.astype(np.float32),hog4.astype(np.float32),  cv2.HISTCMP_CORREL) \n",
        " return score1,score2,score3,score4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DOr7iHVVZM8"
      },
      "outputs": [],
      "source": [
        "!pip install mahotas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmSNRlgSU8rR"
      },
      "outputs": [],
      "source": [
        "def haralick_extractor(pic):  \n",
        "  # importing required libraries\n",
        "  import numpy as np\n",
        "  import mahotas\n",
        "  from pylab import imshow, show\n",
        "\n",
        "  # loading image\n",
        "  img = mahotas.imread(pic)\n",
        "    \n",
        "  # filtering the image\n",
        "  img = img[:, :, 0]\n",
        "    \n",
        "  # setting gaussian filter\n",
        "  gaussian = mahotas.gaussian_filter(img, 15)\n",
        "\n",
        "  # setting threshold value\n",
        "  gaussian = (gaussian > gaussian.mean())\n",
        "\n",
        "  # making is labelled image\n",
        "  labeled, n = mahotas.label(gaussian)\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  # getting haralick features\n",
        "  h_feature = mahotas.features.haralick(labeled)\n",
        "  liste= [*h_feature[0], *h_feature[1], *h_feature[2],*h_feature[3]] \n",
        "  return liste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "513bNqm9hQwZ"
      },
      "outputs": [],
      "source": [
        "def mean_std_bgr(image):\n",
        "  import cv2\n",
        "  img = cv2.imread(image)\n",
        "  Mean = []\n",
        "  standardDev = []\n",
        "  color = ('b','g','r')\n",
        "  for i,col in enumerate(color):\n",
        "    mean, std = cv2.meanStdDev(img[:,:,i])\n",
        "    Mean.append(mean[0][0])\n",
        "    standardDev.append(std[0][0])\n",
        "  return  Mean[0], Mean[1], Mean[2], standardDev[0], standardDev[1], standardDev[2]\n",
        "\n",
        "#finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MBCChG3h37L"
      },
      "outputs": [],
      "source": [
        "#Moyenne de chaque canal et Standard Deviation (HSV)\n",
        "def mean_std_hsv(img):\n",
        "  import cv2\n",
        "  hsv_img = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2HSV)\n",
        "  Mean = []\n",
        "  standardDev = []\n",
        "  channels = ('h','s','v')\n",
        "  for i,channel in enumerate(channels):\n",
        "    mean, std = cv2.meanStdDev(hsv_img[:,:,i])\n",
        "    Mean.append(mean[0][0])\n",
        "    standardDev.append(std[0][0])\n",
        "  return  Mean[0], Mean[1], Mean[2], standardDev[0], standardDev[1], standardDev[2]\n",
        "\n",
        "#finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXernZcwhV8r"
      },
      "outputs": [],
      "source": [
        "#Histogramme des canaux BGR par image\n",
        "def hist_bgr(img):\n",
        "  import cv2\n",
        "  import numpy as np\n",
        "  img=cv2.imread(img)\n",
        "  img1 = cv2.imread(\"ref_verylow.png\")\n",
        "  img2 = cv2.imread(\"ref_low.png\")\n",
        "  img3 = cv2.imread(\"ref_medium.png\")\n",
        "  img4 = cv2.imread(\"ref_high.png\")\n",
        "\n",
        "  hist = []\n",
        "  color = ('b','g','r')\n",
        "  for i,col in enumerate(color):\n",
        "      histr = cv2.calcHist([img],[i],None,[256],[0,256])\n",
        "      histr1 = cv2.calcHist([img1],[i],None,[256],[0,256])\n",
        "      histr2 = cv2.calcHist([img2],[i],None,[256],[0,256])\n",
        "      histr3 = cv2.calcHist([img3],[i],None,[256],[0,256])\n",
        "      histr4 = cv2.calcHist([img4],[i],None,[256],[0,256])\n",
        "\n",
        "  score1=cv2.compareHist(histr.astype(np.float32),histr1.astype(np.float32), cv2.HISTCMP_CORREL )  \n",
        "  score2=cv2.compareHist(histr.astype(np.float32),histr2.astype(np.float32),  cv2.HISTCMP_CORREL)       \n",
        "  score3=cv2.compareHist(histr.astype(np.float32),histr3.astype(np.float32),  cv2.HISTCMP_CORREL)  \n",
        "  score4=cv2.compareHist(histr.astype(np.float32),histr4.astype(np.float32),  cv2.HISTCMP_CORREL)\n",
        "\n",
        "  return score1, score2, score3, score4\n",
        "\n",
        " #finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tchqfz3GiIHk"
      },
      "outputs": [],
      "source": [
        "#Histogramme des canaux HSV par image\n",
        "def hist_hsv(img):\n",
        "  import cv2\n",
        "  import numpy as np\n",
        "\n",
        "  img1 = cv2.cvtColor(cv2.imread(\"ref_verylow.png\"), cv2.COLOR_BGR2HSV)\n",
        "  img2 = cv2.cvtColor(cv2.imread(\"ref_low.png\"), cv2.COLOR_BGR2HSV)\n",
        "  img3 = cv2.cvtColor(cv2.imread(\"ref_medium.png\"), cv2.COLOR_BGR2HSV)\n",
        "  img4 = cv2.cvtColor(cv2.imread(\"ref_high.png\"), cv2.COLOR_BGR2HSV)\n",
        "\n",
        "  hsv_img = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2HSV)\n",
        "  hist = []\n",
        "  channel = ('hc','sm','vy')\n",
        "  for i,ch in enumerate(channel):\n",
        "      histr = cv2.calcHist([hsv_img],[i],None,[256],[0,256])\n",
        "      histr1 = cv2.calcHist([img1],[i],None,[256],[0,256])\n",
        "      histr2 = cv2.calcHist([img2],[i],None,[256],[0,256])\n",
        "      histr3 = cv2.calcHist([img3],[i],None,[256],[0,256])\n",
        "      histr4 = cv2.calcHist([img4],[i],None,[256],[0,256])\n",
        "\n",
        "  score1=cv2.compareHist(histr.astype(np.float32),histr1.astype(np.float32), cv2.HISTCMP_CORREL)  \n",
        "  score2=cv2.compareHist(histr.astype(np.float32),histr2.astype(np.float32),  cv2.HISTCMP_CORREL)       \n",
        "  score3=cv2.compareHist(histr.astype(np.float32),histr3.astype(np.float32),  cv2.HISTCMP_CORREL)  \n",
        "  score4=cv2.compareHist(histr.astype(np.float32),histr4.astype(np.float32),  cv2.HISTCMP_CORREL)\n",
        "\n",
        "  return score1, score2, score3, score4\n",
        "\n",
        " #finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7LNIMb2iMJH"
      },
      "outputs": [],
      "source": [
        "def bgr_distribution(img):\n",
        "  import cv2\n",
        "  import numpy as np\n",
        "  dist = []\n",
        "  # Extract each colour channel\n",
        "  img=cv2.imread(img)\n",
        "  blue, green, red = img[:,:,0], img[:,:,1], img[:,:,2]\n",
        "\n",
        "  # Total red+green+blue intensity\n",
        "  intensity = img.sum(axis=2)\n",
        "\n",
        "  # Function to calculate proportion of a certain channel\n",
        "  def colour_frac(color):\n",
        "      return np.sum(color)/np.sum(intensity)\n",
        "\n",
        "  # Calculate the proportion of each colour\n",
        "  blue_fraction = colour_frac(blue)\n",
        "  green_fraction = colour_frac(green)\n",
        "  red_fraction = colour_frac(red)\n",
        "  \n",
        "  dist.append(blue_fraction)\n",
        "  dist.append(green_fraction)\n",
        "  dist.append(red_fraction)\n",
        "  #sum_colour_fraction = red_fraction + green_fraction + blue_fraction\n",
        "  #print('Red fraction: {}'.format(red_fraction))\n",
        "  #print('\\nGreen fraction: {}'.format(green_fraction))\n",
        "  #print('\\nBlue fraction: {}'.format(blue_fraction))\n",
        "  return blue_fraction, green_fraction, red_fraction\n",
        "\n",
        "#finishied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4usOy_HiiQgm"
      },
      "outputs": [],
      "source": [
        "def image_colorfulness(image):\n",
        "  import cv2\n",
        "  import numpy as np\n",
        "  #rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\t# split the image into its respective RGB components\n",
        "  (B, G, R) = cv2.split(cv2.imread(image).astype(\"float\"))\n",
        "\t# compute rg = R - G\n",
        "  rg = np.absolute(R - G)\n",
        "\t# compute yb = 0.5 * (R + G) - B\n",
        "  yb = np.absolute(0.5 * (R + G) - B)\n",
        "\t# compute the mean and standard deviation of both `rg` and `yb`\n",
        "  (rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
        "  (ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
        "\t# combine the mean and standard deviations\n",
        "  stdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
        "  meanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
        "\t# derive the \"colorfulness\" metric and return it\n",
        "\t#return stdRoot + (0.3 * meanRoot)\n",
        "  return   stdRoot + (0.3 * meanRoot)\n",
        "\n",
        "\n",
        "\n",
        "#finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7HdywdziX11"
      },
      "outputs": [],
      "source": [
        "def contrast(img):\n",
        "  import cv2\n",
        "  img=cv2.imread(img)\n",
        "  gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  contrast = gray_img.std()\n",
        "  return contrast\n",
        "\n",
        "\n",
        "\n",
        "#finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99XqxVKAn_P1"
      },
      "outputs": [],
      "source": [
        "#brightness\n",
        "def brightness(img):\n",
        "   from PIL import Image, ImageStat\n",
        "   im = Image.open(img).convert('L')\n",
        "   stat = ImageStat.Stat(im)\n",
        "   return stat.mean[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGQw8sSyoM_j"
      },
      "outputs": [],
      "source": [
        "#sharpness\n",
        "def sharpness(img): \n",
        "  import cv2\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  import math\n",
        "  img=cv2.imread(img)\n",
        "\n",
        "  def divisors(n):\n",
        "    result = set()\n",
        "    for i in range(1, int(n**0.5)+1):\n",
        "        if n % i == 0:\n",
        "            result.add(i)\n",
        "            result.add(n//i)\n",
        "    return list(result)\n",
        "\n",
        "  def reshape_split(image: np.ndarray, kernel_size: tuple):\n",
        "    img_height, img_width = image.shape\n",
        "    tile_height, tile_width = kernel_size\n",
        "    tiled_array = image.reshape(img_height//tile_height, tile_height, img_width//tile_width, tile_width)\n",
        "    tiled_array = tiled_array.swapaxes(1,2)\n",
        "    return tiled_array\n",
        "\n",
        "  gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  img_blur = cv2.GaussianBlur(gray_img, (3,3), 0)\n",
        "\n",
        "  sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5)\n",
        "  #plt.imshow(sobelxy)\n",
        "  #print(img.shape)\n",
        "  k1 = divisors(sobelxy.shape[0])[1]\n",
        "  k2 = divisors(sobelxy.shape[1])[1]\n",
        "  l1, l2 =(int(sobelxy.shape[0]/k1),int(sobelxy.shape[1]/k2)) \n",
        "  #gives 2x2 images\n",
        "  tiles = reshape_split(gray_img, (l1, l2))\n",
        "\n",
        "  #print(tiles)\n",
        "  max_images = []\n",
        "  min_images = []\n",
        "  EME = 0\n",
        "  for i in range(tiles.shape[0]): #go through rows of pictures (in case of 4 pictures: 2 rows)\n",
        "    for j in range(tiles.shape[1]):#go through columns of pictures (in case of 4 pictures: 2 columns)\n",
        "      max_rows = []\n",
        "      min_rows = []\n",
        "      for row in range(tiles.shape[2]):\n",
        "        max_rows.append(max(tiles[i][j][row]))\n",
        "        min_rows.append(min((tiles[i][j][row])))\n",
        "      max_images.append(max(max_rows))\n",
        "      min_images.append(min(min_rows))\n",
        "\n",
        "  #print(max_images, min_images)\n",
        "  k1 = tiles.shape[0]\n",
        "  k2 = tiles.shape[1]\n",
        "\n",
        "  for i in range(k1*k2):\n",
        "    \n",
        "      if max_images[i] == 0:\n",
        "        a = 1 \n",
        "      else: \n",
        "        a = max_images[i]\n",
        "      if min_images[i] == 0:\n",
        "        b = 1 \n",
        "      else: \n",
        "        b = min_images[i]\n",
        "      EME = EME + math.log(a/b)\n",
        "      EME = (EME*2)/k1*k2\n",
        "\n",
        "  return EME\n",
        "\n",
        "sharpness(\"ref_high.png\")\n",
        "\n",
        "#finished"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgKQGKe1VjPT"
      },
      "source": [
        "putting it all in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvtUKBeBClj"
      },
      "outputs": [],
      "source": [
        "#connexion au google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ermACvRKOKJN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "dataset_path=\"/content/drive/MyDrive/denoised/\"\n",
        "path_very_low=\"/content/drive/MyDrive/denoised/very_Low_Turbidity/\"\n",
        "path_low=\"/content/drive/MyDrive/denoised/Low_Turbidity/\"\n",
        "\n",
        "path_med=\"/content/drive/MyDrive/denoised/Medium_Turbidity/\"\n",
        "path_high=\"/content/drive/MyDrive/denoised/High_Turbidity/\"\n",
        "\n",
        "very_low_dir=os.path.join(path_very_low)\n",
        "low_dir = os.path.join(path_low)\n",
        "\n",
        "# Directory with Medium_Turbidity images\n",
        "medium_dir = os.path.join(path_med)\n",
        "\n",
        "# Directory with High_Turbidity images\n",
        "high_dir = os.path.join(path_high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKjf5vGjOb-G"
      },
      "outputs": [],
      "source": [
        "vld=len(os.listdir(very_low_dir))\n",
        "ld=len(os.listdir(low_dir))\n",
        "md=len(os.listdir(medium_dir))\n",
        "hd=len(os.listdir(high_dir))\n",
        "print('total very_Low_Turbidity images:', vld)\n",
        "print('total Low_Turbidity images:', ld)\n",
        "print('total Medium_Turbidity images:', md)\n",
        "print('total High_Turbidity images:', hd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMn-sz3LOpqw"
      },
      "outputs": [],
      "source": [
        "from mahotas.labeled import label\n",
        "import pandas as pd\n",
        "arr_df=pd.DataFrame(columns=[['Angular Second Moment D1','Contrast D1','Correlation D1','Variance D1','Inverse Difference Moment D1','Sum Average D1','Sum Variance D1','Sum Entropy D1','Entropy D1','Difference Variance D1','Difference Entropy D1','Measure of correlation1 D1','Measure of correlation 2 D1',\n",
        "                                         'Angular Second Moment D2','Contrast D2','Correlation D2','Variance D2','Inverse Difference Moment D2','Sum Average D2','Sum Variance D2','Sum Entropy D2','Entropy D2','Difference Variance D2','Difference Entropy D2','Measure of correlation1 D2','Measure of correlation 2 D2',\n",
        "                                         'Angular Second Moment D3','Contrast D3','Correlation D3','Variance D3','Inverse Difference Moment D3','Sum Average D3','Sum Variance D3','Sum Entropy D3','Entropy D3','Difference Variance D3','Difference Entropy D3','Measure of correlation1 D3','Measure of correlation 2 D3',\n",
        "                                         'Angular Second Moment D4','Contrast D4','Correlation D4','Variance D4','Inverse Difference Moment D4','Sum Average D4','Sum Variance D4','Sum Entropy D4','Entropy D4','Difference Variance D4','Difference Entropy D4','Measure of correlation1 D4','Measure of correlation 2 D4',\n",
        "                                         'lbp similarity class1','lbp similarity class2','lbp similarity class3','lbp similarity class4','HOG Similarity class1','HOG Similarity class2','HOG Similarity class3','HOG Similarity class4','Mean Blue','Mean Green','Mean Red','Sd Blue','Sd Green','Sd Red','mean hue','mean saturation','mean light','sd hue','sd saturation','sd light','Similarity BGR Class 1','Similarity BGR Class 2','Similarity BGR Class 3','Similarity BGR Class 4','Similarity HSV Class 1','Similarity HSV Class 2','Similarity HSV Class 3','Similarity HSV Class 4','Blue proportion','Green Proportion','Red Proportion','Colourflness','Contrast','Brightness','Sharpness','Label']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJmURu6m9c1p"
      },
      "outputs": [],
      "source": [
        "#adding very low directory images\n",
        "liste=[]\n",
        "for filename in os.listdir(very_low_dir):\n",
        "  input_path=os.path.join(very_low_dir,filename)\n",
        "  d=haralick_extractor(input_path)\n",
        "  ho1,ho2,ho3,ho4=hog_extractor(input_path)\n",
        "  sc1,sc2,sc3,sc4=lbp_extractor(input_path)\n",
        "  m1,m2,m3,sd1,sd2,sd3=mean_std_bgr(input_path)\n",
        "  mh1,mh2,mh3,sh1,sh2,sh3=mean_std_hsv(input_path)\n",
        "  s1,s2,s3,s4=hist_bgr(input_path)\n",
        "  so1,so2,so3,so4=hist_hsv(input_path)\n",
        "  b,g,r=bgr_distribution(input_path)\n",
        "  colourflness=image_colorfulness(input_path)\n",
        "  cont=contrast(input_path)\n",
        "  bright=brightness(input_path)\n",
        "  sharp=sharpness(input_path)\n",
        "\n",
        "  liste= [*d,sc1,sc2,sc3,sc4,ho1,ho2,ho3,ho4,m1,m2,m3,sd1,sd2,sd3,mh1,mh2,mh3,sh1,sh2,sh3,s1,s2,s3,s4,so1,so2,so3,so4,b,g,r,colourflness,cont,bright,sharp,1] \n",
        "\n",
        "  arr=pd.Series(data=liste,index=[['Angular Second Moment D1','Contrast D1','Correlation D1','Variance D1','Inverse Difference Moment D1','Sum Average D1','Sum Variance D1','Sum Entropy D1','Entropy D1','Difference Variance D1','Difference Entropy D1','Measure of correlation1 D1','Measure of correlation 2 D1',\n",
        "                                         'Angular Second Moment D2','Contrast D2','Correlation D2','Variance D2','Inverse Difference Moment D2','Sum Average D2','Sum Variance D2','Sum Entropy D2','Entropy D2','Difference Variance D2','Difference Entropy D2','Measure of correlation1 D2','Measure of correlation 2 D2',\n",
        "                                         'Angular Second Moment D3','Contrast D3','Correlation D3','Variance D3','Inverse Difference Moment D3','Sum Average D3','Sum Variance D3','Sum Entropy D3','Entropy D3','Difference Variance D3','Difference Entropy D3','Measure of correlation1 D3','Measure of correlation 2 D3',\n",
        "                                         'Angular Second Moment D4','Contrast D4','Correlation D4','Variance D4','Inverse Difference Moment D4','Sum Average D4','Sum Variance D4','Sum Entropy D4','Entropy D4','Difference Variance D4','Difference Entropy D4','Measure of correlation1 D4','Measure of correlation 2 D4',\n",
        "                                         'lbp similarity class1','lbp similarity class2','lbp similarity class3','lbp similarity class4','HOG Similarity class1','HOG Similarity class2','HOG Similarity class3','HOG Similarity class4','Mean Blue','Mean Green','Mean Red','Sd Blue','Sd Green','Sd Red','mean hue','mean saturation','mean light','sd hue','sd saturation','sd light','Similarity BGR Class 1','Similarity BGR Class 2','Similarity BGR Class 3','Similarity BGR Class 4','Similarity HSV Class 1','Similarity HSV Class 2','Similarity HSV Class 3','Similarity HSV Class 4','Blue proportion','Green Proportion','Red Proportion','Colourflness','Contrast','Brightness','Sharpness','Label']])\n",
        "  print(arr)\n",
        "  arr_df=arr_df.append(arr,ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpENRGcVTcec"
      },
      "outputs": [],
      "source": [
        "#adding low directory images\n",
        "liste=[]\n",
        "for filename in os.listdir(low_dir):\n",
        "  input_path=os.path.join(low_dir,filename)\n",
        "  d=haralick_extractor(input_path)\n",
        "  ho1,ho2,ho3,ho4=hog_extractor(input_path)\n",
        "  sc1,sc2,sc3,sc4=lbp_extractor(input_path)\n",
        "  m1,m2,m3,sd1,sd2,sd3=mean_std_bgr(input_path)\n",
        "  mh1,mh2,mh3,sh1,sh2,sh3=mean_std_hsv(input_path)\n",
        "  s1,s2,s3,s4=hist_bgr(input_path)\n",
        "  so1,so2,so3,so4=hist_hsv(input_path)\n",
        "  b,g,r=bgr_distribution(input_path)\n",
        "  colourflness=image_colorfulness(input_path)\n",
        "  cont=contrast(input_path)\n",
        "  bright=brightness(input_path)\n",
        "  sharp=sharpness(input_path)\n",
        "\n",
        "  liste= [*d,sc1,sc2,sc3,sc4,ho1,ho2,ho3,ho4,m1,m2,m3,sd1,sd2,sd3,mh1,mh2,mh3,sh1,sh2,sh3,s1,s2,s3,s4,so1,so2,so3,so4,b,g,r,colourflness,cont,bright,sharp,2] \n",
        "\n",
        "  arr=pd.Series(data=liste,index=[['Angular Second Moment D1','Contrast D1','Correlation D1','Variance D1','Inverse Difference Moment D1','Sum Average D1','Sum Variance D1','Sum Entropy D1','Entropy D1','Difference Variance D1','Difference Entropy D1','Measure of correlation1 D1','Measure of correlation 2 D1',\n",
        "                                         'Angular Second Moment D2','Contrast D2','Correlation D2','Variance D2','Inverse Difference Moment D2','Sum Average D2','Sum Variance D2','Sum Entropy D2','Entropy D2','Difference Variance D2','Difference Entropy D2','Measure of correlation1 D2','Measure of correlation 2 D2',\n",
        "                                         'Angular Second Moment D3','Contrast D3','Correlation D3','Variance D3','Inverse Difference Moment D3','Sum Average D3','Sum Variance D3','Sum Entropy D3','Entropy D3','Difference Variance D3','Difference Entropy D3','Measure of correlation1 D3','Measure of correlation 2 D3',\n",
        "                                         'Angular Second Moment D4','Contrast D4','Correlation D4','Variance D4','Inverse Difference Moment D4','Sum Average D4','Sum Variance D4','Sum Entropy D4','Entropy D4','Difference Variance D4','Difference Entropy D4','Measure of correlation1 D4','Measure of correlation 2 D4',\n",
        "                                         'lbp similarity class1','lbp similarity class2','lbp similarity class3','lbp similarity class4','HOG Similarity class1','HOG Similarity class2','HOG Similarity class3','HOG Similarity class4','Mean Blue','Mean Green','Mean Red','Sd Blue','Sd Green','Sd Red','mean hue','mean saturation','mean light','sd hue','sd saturation','sd light','Similarity BGR Class 1','Similarity BGR Class 2','Similarity BGR Class 3','Similarity BGR Class 4','Similarity HSV Class 1','Similarity HSV Class 2','Similarity HSV Class 3','Similarity HSV Class 4','Blue proportion','Green Proportion','Red Proportion','Colourflness','Contrast','Brightness','Sharpness','Label']])\n",
        "  print(arr)\n",
        "  arr_df=arr_df.append(arr,ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hACS1ZsSUW2k"
      },
      "outputs": [],
      "source": [
        "#adding high directory images\n",
        "liste=[]\n",
        "for filename in os.listdir(high_dir):\n",
        "  input_path=os.path.join(high_dir,filename)\n",
        "  d=haralick_extractor(input_path)\n",
        "  ho1,ho2,ho3,ho4=hog_extractor(input_path)\n",
        "  sc1,sc2,sc3,sc4=lbp_extractor(input_path)\n",
        "  m1,m2,m3,sd1,sd2,sd3=mean_std_bgr(input_path)\n",
        "  mh1,mh2,mh3,sh1,sh2,sh3=mean_std_hsv(input_path)\n",
        "  s1,s2,s3,s4=hist_bgr(input_path)\n",
        "  so1,so2,so3,so4=hist_hsv(input_path)\n",
        "  b,g,r=bgr_distribution(input_path)\n",
        "  colourflness=image_colorfulness(input_path)\n",
        "  cont=contrast(input_path)\n",
        "  bright=brightness(input_path)\n",
        "  sharp=sharpness(input_path)\n",
        "\n",
        "  liste= [*d,sc1,sc2,sc3,sc4,ho1,ho2,ho3,ho4,m1,m2,m3,sd1,sd2,sd3,mh1,mh2,mh3,sh1,sh2,sh3,s1,s2,s3,s4,so1,so2,so3,so4,b,g,r,colourflness,cont,bright,sharp,4] \n",
        "\n",
        "  arr=pd.Series(data=liste,index=[['Angular Second Moment D1','Contrast D1','Correlation D1','Variance D1','Inverse Difference Moment D1','Sum Average D1','Sum Variance D1','Sum Entropy D1','Entropy D1','Difference Variance D1','Difference Entropy D1','Measure of correlation1 D1','Measure of correlation 2 D1',\n",
        "                                         'Angular Second Moment D2','Contrast D2','Correlation D2','Variance D2','Inverse Difference Moment D2','Sum Average D2','Sum Variance D2','Sum Entropy D2','Entropy D2','Difference Variance D2','Difference Entropy D2','Measure of correlation1 D2','Measure of correlation 2 D2',\n",
        "                                         'Angular Second Moment D3','Contrast D3','Correlation D3','Variance D3','Inverse Difference Moment D3','Sum Average D3','Sum Variance D3','Sum Entropy D3','Entropy D3','Difference Variance D3','Difference Entropy D3','Measure of correlation1 D3','Measure of correlation 2 D3',\n",
        "                                         'Angular Second Moment D4','Contrast D4','Correlation D4','Variance D4','Inverse Difference Moment D4','Sum Average D4','Sum Variance D4','Sum Entropy D4','Entropy D4','Difference Variance D4','Difference Entropy D4','Measure of correlation1 D4','Measure of correlation 2 D4',\n",
        "                                         'lbp similarity class1','lbp similarity class2','lbp similarity class3','lbp similarity class4','HOG Similarity class1','HOG Similarity class2','HOG Similarity class3','HOG Similarity class4','Mean Blue','Mean Green','Mean Red','Sd Blue','Sd Green','Sd Red','mean hue','mean saturation','mean light','sd hue','sd saturation','sd light','Similarity BGR Class 1','Similarity BGR Class 2','Similarity BGR Class 3','Similarity BGR Class 4','Similarity HSV Class 1','Similarity HSV Class 2','Similarity HSV Class 3','Similarity HSV Class 4','Blue proportion','Green Proportion','Red Proportion','Colourflness','Contrast','Brightness','Sharpness','Label']])\n",
        "  print(arr)\n",
        "  arr_df=arr_df.append(arr,ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v41UD_39UlmM"
      },
      "outputs": [],
      "source": [
        "#adding medium directory images\n",
        "liste=[]\n",
        "for filename in os.listdir(medium_dir):\n",
        "  input_path=os.path.join(medium_dir,filename)\n",
        "  d=haralick_extractor(input_path)\n",
        "  ho1,ho2,ho3,ho4=hog_extractor(input_path)\n",
        "  sc1,sc2,sc3,sc4=lbp_extractor(input_path)\n",
        "  m1,m2,m3,sd1,sd2,sd3=mean_std_bgr(input_path)\n",
        "  mh1,mh2,mh3,sh1,sh2,sh3=mean_std_hsv(input_path)\n",
        "  s1,s2,s3,s4=hist_bgr(input_path)\n",
        "  so1,so2,so3,so4=hist_hsv(input_path)\n",
        "  b,g,r=bgr_distribution(input_path)\n",
        "  colourflness=image_colorfulness(input_path)\n",
        "  cont=contrast(input_path)\n",
        "  bright=brightness(input_path)\n",
        "  sharp=sharpness(input_path)\n",
        "\n",
        "  liste= [*d,sc1,sc2,sc3,sc4,ho1,ho2,ho3,ho4,m1,m2,m3,sd1,sd2,sd3,mh1,mh2,mh3,sh1,sh2,sh3,s1,s2,s3,s4,so1,so2,so3,so4,b,g,r,colourflness,cont,bright,sharp,3] \n",
        "\n",
        "  arr=pd.Series(data=liste,index=[['Angular Second Moment D1','Contrast D1','Correlation D1','Variance D1','Inverse Difference Moment D1','Sum Average D1','Sum Variance D1','Sum Entropy D1','Entropy D1','Difference Variance D1','Difference Entropy D1','Measure of correlation1 D1','Measure of correlation 2 D1',\n",
        "                                         'Angular Second Moment D2','Contrast D2','Correlation D2','Variance D2','Inverse Difference Moment D2','Sum Average D2','Sum Variance D2','Sum Entropy D2','Entropy D2','Difference Variance D2','Difference Entropy D2','Measure of correlation1 D2','Measure of correlation 2 D2',\n",
        "                                         'Angular Second Moment D3','Contrast D3','Correlation D3','Variance D3','Inverse Difference Moment D3','Sum Average D3','Sum Variance D3','Sum Entropy D3','Entropy D3','Difference Variance D3','Difference Entropy D3','Measure of correlation1 D3','Measure of correlation 2 D3',\n",
        "                                         'Angular Second Moment D4','Contrast D4','Correlation D4','Variance D4','Inverse Difference Moment D4','Sum Average D4','Sum Variance D4','Sum Entropy D4','Entropy D4','Difference Variance D4','Difference Entropy D4','Measure of correlation1 D4','Measure of correlation 2 D4',\n",
        "                                         'lbp similarity class1','lbp similarity class2','lbp similarity class3','lbp similarity class4','HOG Similarity class1','HOG Similarity class2','HOG Similarity class3','HOG Similarity class4','Mean Blue','Mean Green','Mean Red','Sd Blue','Sd Green','Sd Red','mean hue','mean saturation','mean light','sd hue','sd saturation','sd light','Similarity BGR Class 1','Similarity BGR Class 2','Similarity BGR Class 3','Similarity BGR Class 4','Similarity HSV Class 1','Similarity HSV Class 2','Similarity HSV Class 3','Similarity HSV Class 4','Blue proportion','Green Proportion','Red Proportion','Colourflness','Contrast','Brightness','Sharpness','Label']])\n",
        "  print(arr)\n",
        "  arr_df=arr_df.append(arr,ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teCgTSMMAxLD"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n-7g2T_bdLV"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "arr_df = sklearn.utils.shuffle(arr_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlppfdqSToag"
      },
      "outputs": [],
      "source": [
        "arr_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N4adb8BUi8z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(arr_df.iloc[:,:-1], arr_df.iloc[:,-1:], test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uNJF95tVUOLn"
      },
      "outputs": [],
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf = RandomForestClassifier(max_depth=16, random_state=42,n_estimators=50)\n",
        "clf.fit(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAzlw7GuVWpV"
      },
      "outputs": [],
      "source": [
        "y_pred=clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imcwXfnAVeMp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZkw3CQXdKxp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(clf, X_test, y_test)  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8kf229_WhVx"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypm4A4v8ZzcU"
      },
      "outputs": [],
      "source": [
        "#SVM\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "clf2 = SVC(gamma='auto')\n",
        "clf2.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oAFqTyhaaNs"
      },
      "outputs": [],
      "source": [
        "y_pred2=clf2.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AncppIRAaMR_"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,y_pred2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huIY0PRHfd_N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(clf2, X_test, y_test)  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng7Pbp-PVCca"
      },
      "outputs": [],
      "source": [
        "# XGBOOST\n",
        "from xgboost import XGBClassifier\n",
        "model = XGBClassifier(max_depth=16,n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions for test data\n",
        "\n",
        "y_pred3 = model.predict(X_test)\n",
        "predictions = [round(value) for value in y_pred3]\n",
        "\n",
        "# evaluate predictions\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVex8558fkJH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(clf, X_test, y_test)  \n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copie de aprés réhaussement.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}